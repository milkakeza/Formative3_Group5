{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milkakeza/Formative3_Group5/blob/main/Formative3_Group5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMf6DyV1AzaN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**# New Section"
      ],
      "metadata": {
        "id": "2QXUDN08A0hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AFfJG-zbTMz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2"
      ],
      "metadata": {
        "id": "kpbAbm9XA_GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NICK\n",
        "\n",
        "# Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read data\n",
        "data = pd.read_csv('IMDB Dataset.csv', encoding='utf-8', quotechar='\"')\n",
        "# data\n",
        "\n",
        "# Convert sentiment or category to numeric (positive : 1, negative : 0)\n",
        "data['sentiment'] = data['sentiment'].map({'positive' : 1, 'negative' : 0})\n",
        "\n",
        "# Keyword Selection: Choose keywords that indicate positive and keywords that indicate negative sentiments\n",
        "\n",
        "# Positive keywords (indicate good reviews)\n",
        "positive_keywords = ['loved', 'best', 'amazing', 'enjoyed']\n",
        "# Negative  keywors (indicate bad reviews)\n",
        "negative_keywords = ['boring', 'terrible', 'worst', 'bad']\n",
        "\n",
        "\n",
        "# A function that will find Probabilities for\n",
        "# Prior: P(Positive), Likelihood: P(keyword|Positive), Marginal: P(keyword), Posterior: P(Positive|keyword)\n",
        "def compute_bayes(data, keyword, method_cat):\n",
        "  # Method type: Positive or Negative\n",
        "  if method_cat.lower() == 'positive':\n",
        "    cat = 1\n",
        "  elif method_cat.lower() == 'negative':\n",
        "    cat = 0\n",
        "  else:\n",
        "    print('Method not recognised')\n",
        "\n",
        "  # Prior\n",
        "  total_reviews = len(data)\n",
        "  total_category = len(data[data['sentiment'] == cat])\n",
        "  p_cat = total_category/total_reviews\n",
        "\n",
        "  # Likelihood: P(keyword|Positive)\n",
        "  specific_type_data = data[data['sentiment'] == cat]\n",
        "  count = 0\n",
        "\n",
        "  for review in specific_type_data['review']:\n",
        "    if keyword.lower() in review.lower().split():\n",
        "      count += 1\n",
        "\n",
        "  p_keyword_given_category = count / total_category\n",
        "\n",
        "  # Marginal: P(keyword)\n",
        "  count_whole = 0\n",
        "  for review in data['review']:\n",
        "    if keyword.lower() in review.lower().split():\n",
        "      count_whole += 1\n",
        "\n",
        "  p_keyword = count_whole / total_reviews\n",
        "\n",
        "  # Posterior: P(Positive | keyword) using Bayes' theorem\n",
        "  if p_keyword > 0:\n",
        "    p_category_given_keyword = (p_keyword_given_category * p_cat) / p_keyword\n",
        "  else:\n",
        "    p_category_given_keyword = 0\n",
        "\n",
        "  return{\n",
        "      'method': method_cat,\n",
        "      'Keyword': keyword,\n",
        "      'P(category)': round(p_cat, 4),\n",
        "      'P(keyword|category)': round(p_keyword_given_category, 4),\n",
        "      'P(keyword)': round(p_keyword, 4),\n",
        "      'P(Positive|keyword)': round(p_category_given_keyword, 4)\n",
        "  }\n",
        "\n"
      ],
      "metadata": {
        "id": "StNjT3BcF9hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Probabilities for positive Keywords\n",
        "results_pos = []\n",
        "\n",
        "for kw in positive_keywords:\n",
        "  results_pos.append(compute_bayes(data, kw, 'positive'))\n",
        "\n",
        "# Convert to DataFrame for easy viewing\n",
        "bayes_df = pd.DataFrame(results_pos)\n",
        "bayes_df"
      ],
      "metadata": {
        "id": "WWnGwZK_GiIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Probabilities for Nigative Keywords\n",
        "results_neg = []\n",
        "\n",
        "for kw in negative_keywords:\n",
        "  results_neg.append(compute_bayes(data, kw, 'negative'))\n",
        "\n",
        "# Convert to DataFrame for easy viewing\n",
        "bayes_df = pd.DataFrame(results_neg)\n",
        "bayes_df"
      ],
      "metadata": {
        "id": "SOjcFNtGGlbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GadD36GsHBQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Gradient Descent Manual Calculation\n",
        "  - Objective:\n",
        "    - You will manually compute three updates of the gradient descent algorithm for the parameters m and b in a simple linear regression model.\n",
        "\n",
        "- Instructions:\n",
        "  - Given the linear equation:\n",
        "    * y = mx + b\n",
        "  - where:\n",
        "\n",
        "    - Initial m = -1\n",
        "    - Initial b = 1\n",
        "    - Learning rate = 0.&\n",
        "    - Given points: (1, 3) and (3, 6)\n",
        "\n",
        "  - Compute the predicted values of y for each data point using the current values of m and b\n",
        "\n",
        "  - Derive the gradient of the cost function j(m, b)\n",
        "  , using Mean Squared Error (MSE). In Essence, show the calculation steps to arrive at the derivative of the cost function\n",
        "\n",
        "Iteratively Update\n",
        " and\n",
        " using gradient descent\n",
        "\n",
        "- The number of times you will update m and b is equal to the number of members that are in each respective group\n",
        "- Show all calculations clearly and include intermediate results after each step.\n",
        "  - Each member must do at least 1 iteration\n",
        "- Describe the trend you observe in the values of m and b. Are they moving towards reducing the error?\n",
        "\n",
        "Submission:\n",
        "Submit a neatly written or typed document with all calculations.\n"
      ],
      "metadata": {
        "id": "IjGcavpYBCS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "id = 'https://drive.google.com/file/d/1-VlFaxbW_wmvu-TB41VjAGjq3fxYGhBl/view?usp=sharing/preview'\n",
        "# Display inline done\n",
        "display(IFrame(id, width=600, height=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "TbSxZzzvGUKy",
        "outputId": "c6a99880-0893-4a52-9440-26e4fe42c490"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7a388db073b0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"500\"\n",
              "            src=\"https://drive.google.com/file/d/1-VlFaxbW_wmvu-TB41VjAGjq3fxYGhBl/view?usp=sharing/preview\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1-VlFaxbW_wmvu-TB41VjAGjq3fxYGhBl/view?usp=sharing"
      ],
      "metadata": {
        "id": "Q6GJwD-DGWFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "eWiQBhYlBFhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Milka this is the one that uses scipy, refer to that\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# --- Step 1: Data ---\n",
        "x = np.array([1, 3])\n",
        "y = np.array([3, 6])\n",
        "\n",
        "# --- Step 2: Define Mean Squared Error (MSE) ---\n",
        "def mse(params, x, y):\n",
        "    m, b = params\n",
        "    y_pred = m * x + b\n",
        "    return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "# --- Step 3: Define Gradients (Derivatives) ---\n",
        "def gradient(params, x, y):\n",
        "    m, b = params\n",
        "    n = len(x)\n",
        "    y_pred = m * x + b\n",
        "\n",
        "    # Partial derivatives (∂MSE/∂m and ∂MSE/∂b)\n",
        "    dm = (-2/n) * np.sum(x * (y - y_pred))\n",
        "    db = (-2/n) * np.sum(y - y_pred)\n",
        "\n",
        "    return np.array([dm, db])\n",
        "\n",
        "# --- Step 4: Initialize Parameters ---\n",
        "initial_params = np.array([-1.0, 1.0])  # m and b\n",
        "learning_rate = 0.1\n",
        "max_iterations = 4\n",
        "\n",
        "# --- Step 5: Tracking Variables ---\n",
        "history = {\"m\": [], \"b\": [], \"mse\": []}\n",
        "\n",
        "# --- Step 6: Define Callback Function to Track Progress ---\n",
        "def callback(params):\n",
        "    m, b = params\n",
        "    current_mse = mse(params, x, y)\n",
        "    history[\"m\"].append(m)\n",
        "    history[\"b\"].append(b)\n",
        "    history[\"mse\"].append(current_mse)\n",
        "    print(f\"Iteration {len(history['m'])}: m = {m:.4f}, b = {b:.4f}, MSE = {current_mse:.4f}\")\n",
        "\n",
        "# --- Step 7: Optimization Using SciPy's Conjugate Gradient (CG) ---\n",
        "result = minimize(\n",
        "    mse,\n",
        "    initial_params,\n",
        "    args=(x, y),\n",
        "    jac=gradient,\n",
        "    method='CG',          # Conjugate Gradient (uses the gradients)\n",
        "    callback=callback,\n",
        "    options={'maxiter': max_iterations, 'gtol': 1e-6}\n",
        ")\n",
        "\n",
        "# --- Step 8: Final Results ---\n",
        "final_m, final_b = result.x\n",
        "final_mse = mse(result.x, x, y)\n",
        "print(\"\\n✅ Optimization Completed!\")\n",
        "print(f\"Final m = {final_m:.4f}\")\n",
        "print(f\"Final b = {final_b:.4f}\")\n",
        "print(f\"Final MSE = {final_mse:.4f}\")\n",
        "\n",
        "# --- Step 9: Final Predictions ---\n",
        "final_y_pred = final_m * x + final_b\n",
        "print(\"Final Predictions:\", final_y_pred)\n",
        "\n",
        "# --- Step 10: Visualization ---\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Plot 1: m and b evolution\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history[\"m\"], label='m values', marker='o')\n",
        "plt.plot(history[\"b\"], label='b values', marker='s')\n",
        "plt.title(\"Change in m and b over Iterations (SciPy)\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: MSE over iterations\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history[\"mse\"], color='red', marker='o', label='MSE')\n",
        "plt.title(\"Error (MSE) over Iterations (SciPy)\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Mm8WFym2Ffp4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}